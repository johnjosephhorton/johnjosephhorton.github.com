---
title: All public government data should be easily machine readable
description: >
    John J. Horton
created: !!timestamp '2011-10-2 10:13:00'
tags:
    - Statistics
    - Python
    - Programming

---


{% mark excerpt -%}
The Bureau of Labor Statistics (BLS) has an annual budget of over $640 million (FY 2011), 
a budget  they use to create and then distribute detailed labor market data and analysis to policy makers, 
researchers, journalists and the general public. I can't speak to the "creation" part of their mission, 
but on the "distribution" part, the are failing---organizations with tiny fractions of their resources do a far better job.  
{%- endmark %}

It's not the case that government IT is invariably bad---the Federal Reserve Bank of St. Louis has an amazing 
interface (FRED) and [API](http://api.stlouisfed.org/docs/fred/) for working with their data. 
Unfortunately, not all government statistics are available here, especially some of the more interesting BLS series.  


The essential problem with BLS is that all of their work products---reports, tables etc.---are 
designed to be printed out, not accessed electronically. Many BLS tables are embedded in PDFs, 
which makes the data they contain essentially impossible to extract; non-PDF, text-based tables, 
which are better, are difficult to parse electronically: structure is conveyed by tabs and white space, 
column headings are split over multiple lines with no separators; heading lengths vary etc.  

Why does it matter? For one, when users can access data electronically, via an API, 
they can combine it with other sources, look for patterns, test hypotheses, find bugs / 
measurement errors, create visualization and do all sorts of other things that make the data more useful.  
  
---

BLS does offer a GUI tool for downloading data, but it's kludgy, requires a Java Applet, 
requires series to be hand-selected and then returns an Excel(!) spreadsheet w/ extraneous headers 
and formatting. Furthermore, it's not clear what series and what transformations are needed from GUI-data to 
make the more refined, aggregated tables.  

---  
  
To illustrate how hard it is to get the data out, I wrote a python script to extract the results this table 
(which shows the expected and estimated changes in employment for a number of industries). 
What I wanted to do was make this, which I think is far easier to understand than the table alone:  

![Image](https://img.skitch.com/20111002-t9f89cdsp7cnewqcmf63wd1s3y.jpg)  

To actually create this figure, I needed to get data into in R by way of a CSV file.  The code required to get table data into a useful CSV file, while not rocket science, isn't trivial---there's lots of one-off/hacky things to work around the limitations of the table. Getting the nested structure of the industries e.g., ("Durable Goods" is a subset of "Manufacturing" and "Durable Goods" has 4 sub-classifications) required recursion (see the "bread_crumb" function). FWIW, here's the code:  

    import urllib2
    import csv

    FIRST_LINE = 11
    LAST_LINE = 38

    def get_level(l):
        for i, char in enumerate(l):
            if char != " ":
                break
        return i

    def clean_line(l):
        l = l.replace("\r","")
        l = l.replace("\n","")
        l = l.split(" ")
        return [y.strip() for y in l if y!=""]

    f = urllib2.urlopen("ftp://ftp.bls.gov/pub/suppl/empsit.tab1.txt")
    lines = [line for line in f][FIRST_LINE:LAST_LINE]
    levels = map(get_level, lines)
    data_rows = map(clean_line, lines)
    headings = [y[0] for y in data_rows]

    d_order = dict(zip(headings, range(len(headings))))
    d_level = dict(zip(headings, levels))

    def one_up(heading):
        candidates = headings[:d_order[heading]]
        heading_level = d_level[heading]
        candidates.reverse()
        for c in candidates:
            if d_level[c] < heading_level:
                return c
        else:
            return None

    def crumb_trail(heading):
        if one_up(heading) is None:
            return [heading]
        else:
            return crumb_trail(one_up(heading)) + [heading]

    crumb_trails = map(crumb_trail, headings)
    max_depth = max(map(len, crumb_trails))

    g = open("bls.csv", "w")
    header = ["level_%s" % i for i in range(max_depth)] + [
        "depth", "level", "normal_seasonal_movement", "estimated_over_month_change",
     "sa_adjusted_over_month_change"]

    hier = dict(zip(set(levels), range(len(set(levels)))))
    def get_hier(heading): return hier[d_level[heading]]

    out = csv.writer(g)
    out.writerow(header)
    for trail, data_row in zip(crumb_trails, data_rows):
        industry = [None for i in range(max_depth)]
        indices = map(get_hier, trail)
        depth = max(indices)
        for name, i in zip(trail, indices):
            industry[i] = name
        out.writerow(industry + [depth] + data_row[:3])
    g.close() 


Most of the code is dealing with the problems shows in this sketch:  

![Image](https://img.skitch.com/20111002-r81md92f22sqcfp15m6kdisai9.jpg)  

My suggestion: BLS should borrow someone from FRED and help them create a proper API.  



